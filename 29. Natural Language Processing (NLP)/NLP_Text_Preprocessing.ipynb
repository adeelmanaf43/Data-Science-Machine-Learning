{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization Practicals"
      ],
      "metadata": {
        "id": "Rts8CAgkRNhl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEWOKHWxRAWb",
        "outputId": "6e8a214a-6a96-40fa-9cdb-bc3786f23cba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"\"\"Hello Welcome, to Adeel Manaf's NLP Tutorials.\n",
        "Please do watch the entire course! to become expert in NLP.\n",
        "\"\"\"\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVmdZYXTRVMI",
        "outputId": "22b6963f-8d5a-4b32-ac24-7eb10b1af039"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Welcome, to Adeel Manaf's NLP Tutorials.\n",
            "Please do watch the entire course! to become expert in NLP.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5M35qpvSQtJ",
        "outputId": "2e5dc86f-e65f-4558-ecb3-caddb86e0992"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Tokenization\n",
        "\n",
        "## Paragraph -> sentence\n",
        "from nltk.tokenize import sent_tokenize\n",
        "documents = sent_tokenize(corpus)"
      ],
      "metadata": {
        "id": "hZJtnJoaRwjL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VLrGLS5R6_f",
        "outputId": "baad1dce-d65f-4e6e-8904-67fb34bc8724"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Hello Welcome, to Adeel Manaf's NLP Tutorials.\",\n",
              " 'Please do watch the entire course!',\n",
              " 'to become expert in NLP.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFfIfq3ZSi7J",
        "outputId": "17543ed5-5bc9-4152-e25d-1fd0f6b7e430"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Tokenization\n",
        "## Paragraph --> Words\n",
        "from nltk.tokenize import word_tokenize\n",
        "words = word_tokenize(corpus)\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OGOvRMFSkLs",
        "outputId": "fb21e9e5-253c-4b9e-a140-f5507c0d5794"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Welcome',\n",
              " ',',\n",
              " 'to',\n",
              " 'Adeel',\n",
              " 'Manaf',\n",
              " \"'s\",\n",
              " 'NLP',\n",
              " 'Tutorials',\n",
              " '.',\n",
              " 'Please',\n",
              " 'do',\n",
              " 'watch',\n",
              " 'the',\n",
              " 'entire',\n",
              " 'course',\n",
              " '!',\n",
              " 'to',\n",
              " 'become',\n",
              " 'expert',\n",
              " 'in',\n",
              " 'NLP',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Sentence --> Words\n",
        "for sent in documents:\n",
        "    print(word_tokenize(sent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBqfQXvOSrdZ",
        "outputId": "897e8bb5-05ef-41f0-f538-14fd5e20e980"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'Welcome', ',', 'to', 'Adeel', 'Manaf', \"'s\", 'NLP', 'Tutorials', '.']\n",
            "['Please', 'do', 'watch', 'the', 'entire', 'course', '!']\n",
            "['to', 'become', 'expert', 'in', 'NLP', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize\n",
        "wordpunct_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ji0LPtn8SykX",
        "outputId": "5b674a18-4b43-4dc4-fad1-ac86905c602a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Welcome',\n",
              " ',',\n",
              " 'to',\n",
              " 'Adeel',\n",
              " 'Manaf',\n",
              " \"'\",\n",
              " 's',\n",
              " 'NLP',\n",
              " 'Tutorials',\n",
              " '.',\n",
              " 'Please',\n",
              " 'do',\n",
              " 'watch',\n",
              " 'the',\n",
              " 'entire',\n",
              " 'course',\n",
              " '!',\n",
              " 'to',\n",
              " 'become',\n",
              " 'expert',\n",
              " 'in',\n",
              " 'NLP',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "tokenizer.tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEebi23KTDbf",
        "outputId": "9c1e55ce-17d8-4490-fc71-3bb50d99737d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Welcome',\n",
              " ',',\n",
              " 'to',\n",
              " 'Adeel',\n",
              " 'Manaf',\n",
              " \"'s\",\n",
              " 'NLP',\n",
              " 'Tutorials.',\n",
              " 'Please',\n",
              " 'do',\n",
              " 'watch',\n",
              " 'the',\n",
              " 'entire',\n",
              " 'course',\n",
              " '!',\n",
              " 'to',\n",
              " 'become',\n",
              " 'expert',\n",
              " 'in',\n",
              " 'NLP',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I can't believe it's not butter! It costs $12.99 in the U.S.A.\"\n",
        "print(word_tokenize(text))"
      ],
      "metadata": {
        "id": "JCQHNTCcTORd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c7121ce-bd93-449c-eed3-e7b3d3b8c883"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'ca', \"n't\", 'believe', 'it', \"'s\", 'not', 'butter', '!', 'It', 'costs', '$', '12.99', 'in', 'the', 'U.S.A', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming\n",
        "Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes to the roots of workds known as lemma. Stemming is important in natural language understanding (NLU) and natural language processing (NLP)\n",
        "\n"
      ],
      "metadata": {
        "id": "1aI2T1x8FlEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## classifiation problem\n",
        "\n",
        "## Comments of product is a positive review or negative review\n",
        "\n",
        "### Reviews ----> eating, eat, eaten  [going, gone, goes]\n",
        "\n",
        "words = [\"eating\", \"eats\", \"eaten\", \"writing\", \"writes\", \"programming\", \"programs\", \"history\", \"finally\", \"finalized\"]"
      ],
      "metadata": {
        "id": "C-XtaxqAEapu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PorterStemmer\""
      ],
      "metadata": {
        "id": "zb5Qpzx3Gnqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemming = PorterStemmer()"
      ],
      "metadata": {
        "id": "nWGi-wa_GnGd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "    print(word +\"----> \"+stemming.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyTo6xAkGi70",
        "outputId": "28cb461f-a193-4645-e7c4-8240f18dc70e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating----> eat\n",
            "eats----> eat\n",
            "eaten----> eaten\n",
            "writing----> write\n",
            "writes----> write\n",
            "programming----> program\n",
            "programs----> program\n",
            "history----> histori\n",
            "finally----> final\n",
            "finalized----> final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming.stem('congratulations')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "wPNhocAPGxVx",
        "outputId": "581db9e1-be7b-41ca-8094-f23baf0d14c0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'congratul'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming.stem('sitting')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "mjWMA3YqG73g",
        "outputId": "4ffa39cb-ec02-43ee-8168-776f9ead580c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sit'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RegexpStemmer class\n",
        "NLTK has RegexpStemmer class with the help of which we can easily implement regular expression stemmer algorithms. It basically takes a single regular expression and remove prefix or suffix that matches the expression. Let us see an example."
      ],
      "metadata": {
        "id": "D9Y0FHVuHJ6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "reg_stemmer = nltk.RegexpStemmer('ing$|s$|e$|able$', min = 4)"
      ],
      "metadata": {
        "id": "vyjw63hxG-Mb"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer.stem(\"eating\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "K2XS0vPjHjFT",
        "outputId": "7c6d46d7-d05a-471b-c78e-18ddca9b86fe"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer.stem(\"understandable\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "9vcd3ZmSHk8r",
        "outputId": "5604ebc3-8e9f-450c-90b8-b2ec555c20c1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'understand'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SnowBall Stemmer"
      ],
      "metadata": {
        "id": "8UKJPGgkH4-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snowball_stem = SnowballStemmer('english')\n",
        "for word in words:\n",
        "    print(word+\"--->\" +snowball_stem.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNvE6K3JH2iF",
        "outputId": "59798d6e-da08-411f-9c55-30662f2a4f6c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating--->eat\n",
            "eats--->eat\n",
            "eaten--->eaten\n",
            "writing--->write\n",
            "writes--->write\n",
            "programming--->program\n",
            "programs--->program\n",
            "history--->histori\n",
            "finally--->final\n",
            "finalized--->final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming.stem(\"fairly\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "JB3CCHE6IHM0",
        "outputId": "315bc4de-b11e-4dbd-c8e8-8ffadb31e8af"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'fairli'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming.stem(\"sportingly\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "S0C23zsHIR4X",
        "outputId": "093edb79-927c-481f-cce0-f80dcd23113a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sportingli'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "snowball_stem.stem(\"fairly\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "gmiwWcr5Ipvp",
        "outputId": "0aa6faaa-591d-4948-e9e5-ce471b956edd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'fair'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "snowball_stem.stem(\"sportingly\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Hg2XuRNIItLd",
        "outputId": "f292c158-9d3c-4097-dc86-4ccbdb47f70b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sport'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Processing Using Lemmatization\n",
        "\n",
        "## Wordnet Lemmatizer\n",
        "\n",
        "Lemmatization technique is like stemming. The output we will get after lemmatization is called lemma, which is a root word rather than root stem, the output of stemming. After lemmatization we will be getting a valid word that means the same thing.\n",
        "\n",
        "NLTK provides WordNetLemmatizer class which is a thin wrapper around wordnet corpus. This class uses morphy() function to the WordNet CourpusReader class to find a lemma. Let us understand it with an example."
      ],
      "metadata": {
        "id": "GoLgbHJDJGxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCBUYwZGKIQ1",
        "outputId": "4e600a50-9046-4439-bd35-6e5a1592f7f2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "POS-Noun-n\n",
        "verb-v\n",
        "adjectice-a\n",
        "adverb-r\n",
        "\"\"\"\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemma = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "mYq5qg52Ivkc"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#noun\n",
        "lemma.lemmatize(\"going\", pos = \"n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "U87fiDMSKFeH",
        "outputId": "d7624e8e-9a51-4600-d3d5-6f118053481b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'going'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#verb\n",
        "lemma.lemmatize(\"going\", pos = \"v\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "kfmt9sZfKYaO",
        "outputId": "4e87325f-863d-4ea5-b739-d83c57951da5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'go'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "    print(word+\"----> \"+lemma.lemmatize(word, pos = \"v\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lk12LlI0Kajc",
        "outputId": "10a805dd-43e4-433b-80bf-097d91314bab"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating----> eat\n",
            "eats----> eat\n",
            "eaten----> eat\n",
            "writing----> write\n",
            "writes----> write\n",
            "programming----> program\n",
            "programs----> program\n",
            "history----> history\n",
            "finally----> finally\n",
            "finalized----> finalize\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemma.lemmatize(\"goes\", pos = \"v\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "2zjuUCzCKf0q",
        "outputId": "1ddb4c3c-c517-45ee-8aee-9df66f3205c6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'go'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemma.lemmatize(\"fairly\", pos = \"v\"), lemma.lemmatize(\"sportingly\", pos = \"v\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVLpGZ4DIMyn",
        "outputId": "41a27211-a39e-45c6-a58c-d388b99f4124"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fairly', 'sportingly')"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preporcessing (Stopwords)"
      ],
      "metadata": {
        "id": "At-P_T6bInk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"\"\" I have three visions for India. In 3000 years of our history, people from all over the world have come and invaded us—captured our lands, conquered our minds. Yet, we have never invaded another nation.\n",
        "We have not tried to conquer others. But we have fought for our freedom, and we must protect and nurture it.India must stand up to the world.\n",
        "Unless India stands up, no one will respect us. Only strength respects strength. We must be a developed nation, not a developing nation.Why are we, as a nation, so obsessed with foreign goods? We must be self-reliant. We have the talent, the resources, and the capability to be leaders in science, technology, and innovation.Dream big! Have the courage to think differently and to invent, to travel unexplored paths, to conquer the problems, and to succeed.Believe in yourself, work hard, and make India a developed nation!\"\"\""
      ],
      "metadata": {
        "id": "HMhhb2dJIWgh"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "IoDtulsTJ6kI"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLay5ulyJ_pE",
        "outputId": "85b9820b-461c-4740-fe49-c31a7c29015f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMcbjaSpKBzP",
        "outputId": "c5e60d2d-54aa-490e-c3e9-16089d1831e0"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " \"he'd\",\n",
              " \"he'll\",\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " \"he's\",\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " \"i'd\",\n",
              " 'if',\n",
              " \"i'll\",\n",
              " \"i'm\",\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it'd\",\n",
              " \"it'll\",\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " \"i've\",\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she'd\",\n",
              " \"she'll\",\n",
              " \"she's\",\n",
              " 'should',\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " \"should've\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " \"they'd\",\n",
              " \"they'll\",\n",
              " \"they're\",\n",
              " \"they've\",\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " \"we'd\",\n",
              " \"we'll\",\n",
              " \"we're\",\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " \"we've\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " 'your',\n",
              " \"you're\",\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " \"you've\"]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "1gYwb4VoKImD"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ikjd_iWaKfyy",
        "outputId": "be25cc95-1eb0-4cdb-caa8-7e048f5462f2"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' I have three visions for India.',\n",
              " 'In 3000 years of our history, people from all over the world have come and invaded us—captured our lands, conquered our minds.',\n",
              " 'Yet, we have never invaded another nation.',\n",
              " 'We have not tried to conquer others.',\n",
              " 'But we have fought for our freedom, and we must protect and nurture it.India must stand up to the world.',\n",
              " 'Unless India stands up, no one will respect us.',\n",
              " 'Only strength respects strength.',\n",
              " 'We must be a developed nation, not a developing nation.Why are we, as a nation, so obsessed with foreign goods?',\n",
              " 'We must be self-reliant.',\n",
              " 'We have the talent, the resources, and the capability to be leaders in science, technology, and innovation.Dream big!',\n",
              " 'Have the courage to think differently and to invent, to travel unexplored paths, to conquer the problems, and to succeed.Believe in yourself, work hard, and make India a developed nation!']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Apply STopwords and filter and then apply stemming\n",
        "porter_stemmer = []\n",
        "for i in range(len(sentences)):\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words(\"english\"))]\n",
        "    porter_stemmer.append(\" \".join(words))\n",
        "porter_stemmer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzpSwyzAKo6A",
        "outputId": "be138193-c6d5-41f1-c86c-4f72396282be"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i three vision india .',\n",
              " 'in 3000 year histori , peopl world come invad us—captur land , conquer mind .',\n",
              " 'yet , never invad anoth nation .',\n",
              " 'we tri conquer other .',\n",
              " 'but fought freedom , must protect nurtur it.india must stand world .',\n",
              " 'unless india stand , one respect us .',\n",
              " 'onli strength respect strength .',\n",
              " 'we must develop nation , develop nation.whi , nation , obsess foreign good ?',\n",
              " 'we must self-reli .',\n",
              " 'we talent , resourc , capabl leader scienc , technolog , innovation.dream big !',\n",
              " 'have courag think differ invent , travel unexplor path , conquer problem , succeed.believ , work hard , make india develop nation !']"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# applying snowballstemmer\n",
        "snowball_stemmer = SnowballStemmer(language=\"english\")\n",
        "## Apply STopwords and filter and then apply stemming\n",
        "snowball_sent = []\n",
        "for i in range(len(sentences)):\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "    words = [snowball_stem.stem(word) for word in words if word not in set(stopwords.words(\"english\"))]\n",
        "    snowball_sent.append(\" \".join(words))\n",
        "\n",
        "snowball_sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGv6AvgFLFEB",
        "outputId": "7dc3e1dc-39b2-4986-de37-228800a47634"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i three vision india .',\n",
              " 'in 3000 year histori , peopl world come invad us—captur land , conquer mind .',\n",
              " 'yet , never invad anoth nation .',\n",
              " 'we tri conquer other .',\n",
              " 'but fought freedom , must protect nurtur it.india must stand world .',\n",
              " 'unless india stand , one respect us .',\n",
              " 'onli strength respect strength .',\n",
              " 'we must develop nation , develop nation.whi , nation , obsess foreign good ?',\n",
              " 'we must self-reli .',\n",
              " 'we talent , resourc , capabl leader scienc , technolog , innovation.dream big !',\n",
              " 'have courag think differ invent , travel unexplor path , conquer problem , succeed.believ , work hard , make india develop nation !']"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# applying lemmatization\n",
        "lemma = WordNetLemmatizer()\n",
        "## Apply STopwords and filter and then apply stemming\n",
        "lemma_sent = []\n",
        "for i in range(len(sentences)):\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "    words = [lemma.lemmatize(word.lower(), pos = 'v') for word in words if word not in set(stopwords.words(\"english\"))]\n",
        "    lemma_sent.append(\" \".join(words))\n",
        "\n",
        "lemma_sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHryfHrPLufH",
        "outputId": "80a375f7-fe0f-4ae7-faa3-d489f53cf85c"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i three visions india .',\n",
              " 'in 3000 years history , people world come invade us—captured land , conquer mind .',\n",
              " 'yet , never invade another nation .',\n",
              " 'we try conquer others .',\n",
              " 'but fight freedom , must protect nurture it.india must stand world .',\n",
              " 'unless india stand , one respect us .',\n",
              " 'only strength respect strength .',\n",
              " 'we must develop nation , develop nation.why , nation , obsess foreign goods ?',\n",
              " 'we must self-reliant .',\n",
              " 'we talent , resources , capability leaders science , technology , innovation.dream big !',\n",
              " 'have courage think differently invent , travel unexplored paths , conquer problems , succeed.believe , work hard , make india develop nation !']"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part of Speech Tagging Using NLTK"
      ],
      "metadata": {
        "id": "sCvIRJKFNC8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T66wFTUIMiKa",
        "outputId": "8808b903-1298-4000-b9d2-38e6780af37e"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' I have three visions for India.',\n",
              " 'In 3000 years of our history, people from all over the world have come and invaded us—captured our lands, conquered our minds.',\n",
              " 'Yet, we have never invaded another nation.',\n",
              " 'We have not tried to conquer others.',\n",
              " 'But we have fought for our freedom, and we must protect and nurture it.India must stand up to the world.',\n",
              " 'Unless India stands up, no one will respect us.',\n",
              " 'Only strength respects strength.',\n",
              " 'We must be a developed nation, not a developing nation.Why are we, as a nation, so obsessed with foreign goods?',\n",
              " 'We must be self-reliant.',\n",
              " 'We have the talent, the resources, and the capability to be leaders in science, technology, and innovation.Dream big!',\n",
              " 'Have the courage to think differently and to invent, to travel unexplored paths, to conquer the problems, and to succeed.Believe in yourself, work hard, and make India a developed nation!']"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We will find out the part of speech tag\n",
        "#nltk.download('averaged_perceptron_tagger_eng')\n",
        "pos_tag = []\n",
        "for i in range(len(sentences)):\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "    words = [word for word in words if not word in set(stopwords.words('english'))]\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "    print(pos_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLeVE1iNNm3u",
        "outputId": "88b2bad4-a8ae-44ce-c7a3-6830f7f14813"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I', 'PRP'), ('three', 'CD'), ('visions', 'NNS'), ('India', 'NNP'), ('.', '.')]\n",
            "[('In', 'IN'), ('3000', 'CD'), ('years', 'NNS'), ('history', 'NN'), (',', ','), ('people', 'NNS'), ('world', 'NN'), ('come', 'VBP'), ('invaded', 'VBN'), ('us—captured', 'JJ'), ('lands', 'NNS'), (',', ','), ('conquered', 'VBD'), ('minds', 'NNS'), ('.', '.')]\n",
            "[('Yet', 'RB'), (',', ','), ('never', 'RB'), ('invaded', 'VBD'), ('another', 'DT'), ('nation', 'NN'), ('.', '.')]\n",
            "[('We', 'PRP'), ('tried', 'VBD'), ('conquer', 'NN'), ('others', 'NNS'), ('.', '.')]\n",
            "[('But', 'CC'), ('fought', 'JJ'), ('freedom', 'NN'), (',', ','), ('must', 'MD'), ('protect', 'VB'), ('nurture', 'JJ'), ('it.India', 'NN'), ('must', 'MD'), ('stand', 'VB'), ('world', 'NN'), ('.', '.')]\n",
            "[('Unless', 'IN'), ('India', 'NNP'), ('stands', 'VBZ'), (',', ','), ('one', 'CD'), ('respect', 'NN'), ('us', 'PRP'), ('.', '.')]\n",
            "[('Only', 'RB'), ('strength', 'NN'), ('respects', 'NNS'), ('strength', 'NN'), ('.', '.')]\n",
            "[('We', 'PRP'), ('must', 'MD'), ('developed', 'VB'), ('nation', 'NN'), (',', ','), ('developing', 'VBG'), ('nation.Why', 'JJ'), (',', ','), ('nation', 'NN'), (',', ','), ('obsessed', 'VBD'), ('foreign', 'JJ'), ('goods', 'NNS'), ('?', '.')]\n",
            "[('We', 'PRP'), ('must', 'MD'), ('self-reliant', 'VB'), ('.', '.')]\n",
            "[('We', 'PRP'), ('talent', 'VBP'), (',', ','), ('resources', 'NNS'), (',', ','), ('capability', 'NN'), ('leaders', 'NNS'), ('science', 'NN'), (',', ','), ('technology', 'NN'), (',', ','), ('innovation.Dream', 'NN'), ('big', 'JJ'), ('!', '.')]\n",
            "[('Have', 'VBP'), ('courage', 'NN'), ('think', 'VBP'), ('differently', 'RB'), ('invent', 'JJ'), (',', ','), ('travel', 'NN'), ('unexplored', 'JJ'), ('paths', 'NNS'), (',', ','), ('conquer', 'NN'), ('problems', 'NNS'), (',', ','), ('succeed.Believe', 'VB'), (',', ','), ('work', 'VB'), ('hard', 'RB'), (',', ','), ('make', 'VB'), ('India', 'NNP'), ('developed', 'JJ'), ('nation', 'NN'), ('!', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words =  \"Taj Mahal is a beautiful Monument\".split()\n",
        "nltk.pos_tag(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6bPsIcIOJZW",
        "outputId": "439cce88-749a-44b7-cfa2-274123cf81e8"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Taj', 'NNP'),\n",
              " ('Mahal', 'NNP'),\n",
              " ('is', 'VBZ'),\n",
              " ('a', 'DT'),\n",
              " ('beautiful', 'JJ'),\n",
              " ('Monument', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Named Entity Recognition"
      ],
      "metadata": {
        "id": "TxQOB0fRYBxT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sentence = \"\"\"The Eiffel Tower was built from 1887 to 1889 by French engineer Gustave Eiffel, whose company specialized in building metal frameworks and structures.\"\"\"\n",
        "Person EG: Krish C Naik\n",
        "Place Or Location Eg: India\n",
        "Date Eg: September, 24-00-1989\n",
        "Money Eg: 1 million dollar\n",
        "Organization Eg: iNeuron Private Limited\n",
        "Percent Eg: 20%, twenty percent"
      ],
      "metadata": {
        "id": "fYJFnvltYHER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"\"\"The Eiffel Tower was built from 1887 to 1889 by French engineer Gustave Eiffel, whose company specialized in building metal frameworks and structures.\"\"\"\n",
        "words = nltk.word_tokenize(sentence)\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "922XWuVTYGq_",
        "outputId": "a5563229-d3ee-4cef-dd1e-6b90ae0a5cc5"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'Eiffel',\n",
              " 'Tower',\n",
              " 'was',\n",
              " 'built',\n",
              " 'from',\n",
              " '1887',\n",
              " 'to',\n",
              " '1889',\n",
              " 'by',\n",
              " 'French',\n",
              " 'engineer',\n",
              " 'Gustave',\n",
              " 'Eiffel',\n",
              " ',',\n",
              " 'whose',\n",
              " 'company',\n",
              " 'specialized',\n",
              " 'in',\n",
              " 'building',\n",
              " 'metal',\n",
              " 'frameworks',\n",
              " 'and',\n",
              " 'structures',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tags = nltk.pos_tag(words)\n",
        "pos_tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iq4BJDC5Xu8o",
        "outputId": "8c0a765c-3fdd-4155-9532-7d3a5bdb0983"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DT'),\n",
              " ('Eiffel', 'NNP'),\n",
              " ('Tower', 'NNP'),\n",
              " ('was', 'VBD'),\n",
              " ('built', 'VBN'),\n",
              " ('from', 'IN'),\n",
              " ('1887', 'CD'),\n",
              " ('to', 'TO'),\n",
              " ('1889', 'CD'),\n",
              " ('by', 'IN'),\n",
              " ('French', 'JJ'),\n",
              " ('engineer', 'NN'),\n",
              " ('Gustave', 'NNP'),\n",
              " ('Eiffel', 'NNP'),\n",
              " (',', ','),\n",
              " ('whose', 'WP$'),\n",
              " ('company', 'NN'),\n",
              " ('specialized', 'VBD'),\n",
              " ('in', 'IN'),\n",
              " ('building', 'NN'),\n",
              " ('metal', 'NN'),\n",
              " ('frameworks', 'NNS'),\n",
              " ('and', 'CC'),\n",
              " ('structures', 'NNS'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install svgling"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HebRgO1CZRUK",
        "outputId": "17f1fd98-768d-493f-c2a9-6c2ebf037990"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting svgling\n",
            "  Downloading svgling-0.5.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting svgwrite (from svgling)\n",
            "  Downloading svgwrite-1.4.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "Downloading svgling-0.5.0-py3-none-any.whl (31 kB)\n",
            "Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: svgwrite, svgling\n",
            "Successfully installed svgling-0.5.0 svgwrite-1.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#nltk.download('maxent_ne_chunker_tab')\n",
        "#nltk.download('words')\n",
        "named_entity = nltk.ne_chunk(pos_tags)\n",
        "named_entity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "oUf-V90aY2ZF",
        "outputId": "0e762be9-9b6e-460e-c8ff-0d3bdbb7b3fc"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tree('S', [('The', 'DT'), Tree('ORGANIZATION', [('Eiffel', 'NNP'), ('Tower', 'NNP')]), ('was', 'VBD'), ('built', 'VBN'), ('from', 'IN'), ('1887', 'CD'), ('to', 'TO'), ('1889', 'CD'), ('by', 'IN'), Tree('GPE', [('French', 'JJ')]), ('engineer', 'NN'), Tree('PERSON', [('Gustave', 'NNP'), ('Eiffel', 'NNP')]), (',', ','), ('whose', 'WP$'), ('company', 'NN'), ('specialized', 'VBD'), ('in', 'IN'), ('building', 'NN'), ('metal', 'NN'), ('frameworks', 'NNS'), ('and', 'CC'), ('structures', 'NNS'), ('.', '.')])"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,1424.0,168.0\" width=\"1424px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"2.80899%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">The</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"1.40449%\" y1=\"20px\" y2=\"48px\" /><svg width=\"8.42697%\" x=\"2.80899%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ORGANIZATION</text></svg><svg width=\"53.3333%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Eiffel</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.6667%\" y1=\"20px\" y2=\"48px\" /><svg width=\"46.6667%\" x=\"53.3333%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Tower</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.6667%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"7.02247%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.80899%\" x=\"11.236%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">was</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"12.6404%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.93258%\" x=\"14.0449%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">built</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"16.0112%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.37079%\" x=\"17.9775%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">from</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"19.6629%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.37079%\" x=\"21.3483%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">1887</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"23.0337%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.24719%\" x=\"24.7191%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">to</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">TO</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"25.8427%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.37079%\" x=\"26.9663%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">1889</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"28.6517%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.24719%\" x=\"30.3371%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">by</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"31.4607%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.49438%\" x=\"32.5843%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">GPE</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">French</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"34.8315%\" y1=\"20px\" y2=\"48px\" /><svg width=\"5.61798%\" x=\"37.0787%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">engineer</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"39.8876%\" y1=\"20px\" y2=\"48px\" /><svg width=\"9.55056%\" x=\"42.6966%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"52.9412%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Gustave</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.4706%\" y1=\"20px\" y2=\"48px\" /><svg width=\"47.0588%\" x=\"52.9412%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Eiffel</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.4706%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"47.4719%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.68539%\" x=\"52.2472%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"53.0899%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.93258%\" x=\"53.9326%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">whose</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">WP$</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"55.8989%\" y1=\"20px\" y2=\"48px\" /><svg width=\"5.05618%\" x=\"57.8652%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">company</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"60.3933%\" y1=\"20px\" y2=\"48px\" /><svg width=\"7.30337%\" x=\"62.9213%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">specialized</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"66.573%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.24719%\" x=\"70.2247%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">in</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"71.3483%\" y1=\"20px\" y2=\"48px\" /><svg width=\"5.61798%\" x=\"72.4719%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">building</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"75.2809%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.93258%\" x=\"78.0899%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">metal</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"80.0562%\" y1=\"20px\" y2=\"48px\" /><svg width=\"6.74157%\" x=\"82.0225%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">frameworks</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"85.3933%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.80899%\" x=\"88.764%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">and</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CC</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"90.1685%\" y1=\"20px\" y2=\"48px\" /><svg width=\"6.74157%\" x=\"91.573%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">structures</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"94.9438%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.68539%\" x=\"98.3146%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"99.1573%\" y1=\"20px\" y2=\"48px\" /></svg>"
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Steps for NLP Project\n",
        "\n",
        "## Text Preporcessing Part One\n",
        "1. Tokenization\n",
        "2. Convert all words into lowercase.\n",
        "3. Regular Expression\n",
        "\n",
        "## Text Preprocessing Part Two\n",
        "1. Stemming\n",
        "2. Lemmatization\n",
        "3. Stopwords\n",
        "\n",
        "### Convert text into vectors\n",
        "\n",
        "1. One Hot Encoding\n",
        "2. Bag of Words (BOW)\n",
        "3. TF-IDF\n",
        "4. Word2Vec\n",
        "5. Average Word2Vec\n",
        "\n",
        "\n",
        "### ML Algorithms"
      ],
      "metadata": {
        "id": "ihJwQ7jDZ0K9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One Hot Encoding\n",
        "\n",
        "Creating separate variable for each word.\n",
        "\n",
        "### Advantages\n",
        "1. Easy to implement with Python. (sklearn, pandas(get_dummies))\n",
        "2.\n",
        "\n",
        "### Disadvantages\n",
        "\n",
        "1. Create a sparse matrix (lot of 1's and 0's, in most of the machine learning algorithm it leads to overfitting).\n",
        "2. For ML algorithm we need fixed size input, which is not there in OHE.\n",
        "3. There is no semantic meaning getting captured.\n",
        "4. Out of vocabulary\n"
      ],
      "metadata": {
        "id": "BiaaZwlMbAEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1OTfhsFIY7Ki"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}